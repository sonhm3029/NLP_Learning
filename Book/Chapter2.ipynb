{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2. NLP pipeline\n",
    "\n",
    "The key stages in the NLP pipeline:\n",
    "\n",
    "1. Data acquisition\n",
    "2. Text cleaning\n",
    "3. Pre-processing\n",
    "4. Feature engineering\n",
    "5. Modeling\n",
    "6. Evaluation\n",
    "7. Deployment\n",
    "8. Monitoring and model updating\n",
    "\n",
    "![Untitled](Chapter%202%20NLP%20pipeline%20c140380e660240a9a622b98490b31b28/Untitled.png)\n",
    "\n",
    "## 1. Data Acquisition\n",
    "\n",
    "Ways to collecting data:\n",
    "\n",
    "- Use a public dataset\n",
    "- Scrape data: crawl data t·ª´ internet\n",
    "- Product intervention: Thu th·∫≠p data t·ª´ vi·ªác ch·∫°y c√°c s·∫£n ph·∫©m th·ª±c t·∫ø\n",
    "- Data augmentation:\n",
    "    - Synonym replacement: Ch·ªçn ng·∫´u nhi√™n `k` t·ª´ trong c√¢u m√† kh√¥ng ph·∫£i l√† t·ª´ k·∫øt th√∫c c√¢u, replace c√°c t·ª´ n√†y b·∫±ng synonyms c·ªßa ch√∫ng. Ta c√≥ th·ªÉ d√πng Synsets trong Wordnet [3,4]\n",
    "    - Back translation: Gi·∫£ s·ª≠ ta c√≥ S1 b·∫±ng ti·∫øng Anh, ta c√≥ th·ªÉ d√πng google translate ƒë·ªÉ d·ªãch n√≥ sang S2 l√† ti·∫øng ƒê·ª©c, t·ª´ S2 ta l·∫°i d√πng n√≥ ƒë·ªÉ d·ªãch ng∆∞·ª£c l·∫°i ti·∫øng Anh ‚áí 2 c√¢u c√≥ ng·ªØ nghƒ©a l√† gi·ªëng nhau nh∆∞ng c·∫•u tr√∫c c·ªßa n√≥ l·∫°i kh√°c.\n",
    "\n",
    "    ![Untitled](Chapter%202%20NLP%20pipeline%20c140380e660240a9a622b98490b31b28/Untitled%201.png)\n",
    "    - TF-IDF-based word replacement\n",
    "    - Bigram flipping\n",
    "    - Replacing entities\n",
    "    - Adding noise to data\n",
    "    - Advanced techniques:\n",
    "        - Snorkel\n",
    "        - Easy Data Augmentation\n",
    "        - Active Learning\n",
    "\n",
    "## 2. Text extraction and cleanup\n",
    "\n",
    "![Untitled](Chapter%202%20NLP%20pipeline%20c140380e660240a9a622b98490b31b28/Untitled%202.png)\n",
    "\n",
    "### HTML Parsing and Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unicode Normalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'I love \\xf0\\x9f\\x8d\\x95! Shall we book a \\xf0\\x9f\\x9a\\x97 to gizza?'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import emoji\n",
    "\n",
    "text = 'I love üçï! Shall we book a üöó to gizza?'\n",
    "Text = text.encode(\"utf-8\")\n",
    "\n",
    "Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spelling Correction\n",
    "\n",
    "C√≥ th·ªÉ s·ª≠ d·ª•ng REST API cho vi·ªác s·ª≠a l·ªói ch√≠nh t·∫£ c·ªßa Microsoft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"error\": {\n",
      "        \"code\": \"401\",\n",
      "        \"message\": \"Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "api_key = \"\"\n",
    "example_text = \"Hollo, wrld\"\n",
    "endpoint = \"https://api.cognitive.microsoft.com/bing/v7.0/SpellCheck\"\n",
    "\n",
    "data = {\n",
    "    'text': example_text\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'mkt':'en-us',\n",
    "    'mode':'proof'\n",
    "    }\n",
    "\n",
    "headers = {\n",
    "    'Content-Type': 'application/x-www-form-urlencoded',\n",
    "    'Ocp-Apim-Subscription-Key': api_key,\n",
    "    }\n",
    "\n",
    "response = requests.post(endpoint, headers=headers, params=params, data=data)\n",
    "\n",
    "json_response = response.json()\n",
    "print(json.dumps(json_response, indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System-Specific Error Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the nineteenth century the only kind of linguistics considered\n",
      "seriously was this comparative and historical stady of word in angus\n",
      "\n",
      "known or believed to be cagnate‚Äîsay the Semitic languages, or the Indo-\n",
      "European languages. It is significant that the Germans who really made\n",
      "the subject what it was, used the term Indo-germanisch, Those who know\n",
      "the popular works of Otto Jespersen will remember how firmly he\n",
      "declares that linguistic science is historical. And those who have noticed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from pytesseract import image_to_string\n",
    "import pytesseract\n",
    "\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd  = r'C:\\Program Files\\Tesseract-OCR\\tesseract'\n",
    "filename = \"./test_img.png\"\n",
    "\n",
    "text = image_to_string(Image.open(filename))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "## 3. Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common pre-processing steps used in NLP software:\n",
    "\n",
    "- Preliminaries:\n",
    "    - Sentence segmentation and word tokenization\n",
    "- Frequent steps\n",
    "    - Stop word removal\n",
    "    - stemming and lemmatization\n",
    "    - removing digits/punctuation\n",
    "    - lowercasing\n",
    "- Other steps:\n",
    "    - Normalization, lagunage detection, code mixing, transliteration\n",
    "- Advanced processing\n",
    "    - POS tagging, parsing, corereference resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Segmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nIn the previous chapter, we saw examples of some common NLP\\napplications that we might encounter in everyday life.',\n",
       " 'If we were asked to\\nbuild such an application, think about how we would approach doing so at our\\norganization.',\n",
       " 'We would normally walk through the requirements and break the\\nproblem down into several sub-problems, then try to develop a step-by-step\\nprocedure to solve them.',\n",
       " 'Since language processing is involved, we would also\\nlist all the forms of text processing needed at each step.',\n",
       " 'This step-by-step\\nprocessing of text is known as pipeline.',\n",
       " 'It is the series of steps involved in\\nbuilding any NLP model.',\n",
       " 'These steps are common in every NLP project, so it\\nmakes sense to study them in this chapter.',\n",
       " 'Understanding some common procedures\\nin any NLP pipeline will enable us to get started on any NLP problem encountered\\nin the workplace.',\n",
       " 'Laying out and developing a text-processing pipeline is seen\\nas a starting point for any NLP application development process.',\n",
       " 'In this\\nchapter, we will learn about the various steps involved and how they play\\nimportant roles in solving the NLP problem and we‚Äôll see a few guidelines\\nabout when and how to use which step.',\n",
       " 'In later chapters, we‚Äôll discuss\\nspecific pipelines for various NLP tasks (e.g., Chapters 4‚Äì7).r']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "mytext = \"\"\"\n",
    "In the previous chapter, we saw examples of some common NLP\n",
    "applications that we might encounter in everyday life. If we were asked to\n",
    "build such an application, think about how we would approach doing so at our\n",
    "organization. We would normally walk through the requirements and break the\n",
    "problem down into several sub-problems, then try to develop a step-by-step\n",
    "procedure to solve them. Since language processing is involved, we would also\n",
    "list all the forms of text processing needed at each step. This step-by-step\n",
    "processing of text is known as pipeline. It is the series of steps involved in\n",
    "building any NLP model. These steps are common in every NLP project, so it\n",
    "makes sense to study them in this chapter. Understanding some common procedures\n",
    "in any NLP pipeline will enable us to get started on any NLP problem encountered\n",
    "in the workplace. Laying out and developing a text-processing pipeline is seen\n",
    "as a starting point for any NLP application development process. In this\n",
    "chapter, we will learn about the various steps involved and how they play\n",
    "important roles in solving the NLP problem and we‚Äôll see a few guidelines\n",
    "about when and how to use which step. In later chapters, we‚Äôll discuss\n",
    "specific pipelines for various NLP tasks (e.g., Chapters 4‚Äì7).r\n",
    "\"\"\"\n",
    "\n",
    "my_sentences = sent_tokenize(mytext)\n",
    "\n",
    "my_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In the previous chapter, we saw examples of some common NLP\n",
      "applications that we might encounter in everyday life.\n",
      "['In', 'the', 'previous', 'chapter', ',', 'we', 'saw', 'examples', 'of', 'some', 'common', 'NLP', 'applications', 'that', 'we', 'might', 'encounter', 'in', 'everyday', 'life', '.']\n",
      "If we were asked to\n",
      "build such an application, think about how we would approach doing so at our\n",
      "organization.\n",
      "['If', 'we', 'were', 'asked', 'to', 'build', 'such', 'an', 'application', ',', 'think', 'about', 'how', 'we', 'would', 'approach', 'doing', 'so', 'at', 'our', 'organization', '.']\n",
      "We would normally walk through the requirements and break the\n",
      "problem down into several sub-problems, then try to develop a step-by-step\n",
      "procedure to solve them.\n",
      "['We', 'would', 'normally', 'walk', 'through', 'the', 'requirements', 'and', 'break', 'the', 'problem', 'down', 'into', 'several', 'sub-problems', ',', 'then', 'try', 'to', 'develop', 'a', 'step-by-step', 'procedure', 'to', 'solve', 'them', '.']\n",
      "Since language processing is involved, we would also\n",
      "list all the forms of text processing needed at each step.\n",
      "['Since', 'language', 'processing', 'is', 'involved', ',', 'we', 'would', 'also', 'list', 'all', 'the', 'forms', 'of', 'text', 'processing', 'needed', 'at', 'each', 'step', '.']\n",
      "This step-by-step\n",
      "processing of text is known as pipeline.\n",
      "['This', 'step-by-step', 'processing', 'of', 'text', 'is', 'known', 'as', 'pipeline', '.']\n",
      "It is the series of steps involved in\n",
      "building any NLP model.\n",
      "['It', 'is', 'the', 'series', 'of', 'steps', 'involved', 'in', 'building', 'any', 'NLP', 'model', '.']\n",
      "These steps are common in every NLP project, so it\n",
      "makes sense to study them in this chapter.\n",
      "['These', 'steps', 'are', 'common', 'in', 'every', 'NLP', 'project', ',', 'so', 'it', 'makes', 'sense', 'to', 'study', 'them', 'in', 'this', 'chapter', '.']\n",
      "Understanding some common procedures\n",
      "in any NLP pipeline will enable us to get started on any NLP problem encountered\n",
      "in the workplace.\n",
      "['Understanding', 'some', 'common', 'procedures', 'in', 'any', 'NLP', 'pipeline', 'will', 'enable', 'us', 'to', 'get', 'started', 'on', 'any', 'NLP', 'problem', 'encountered', 'in', 'the', 'workplace', '.']\n",
      "Laying out and developing a text-processing pipeline is seen\n",
      "as a starting point for any NLP application development process.\n",
      "['Laying', 'out', 'and', 'developing', 'a', 'text-processing', 'pipeline', 'is', 'seen', 'as', 'a', 'starting', 'point', 'for', 'any', 'NLP', 'application', 'development', 'process', '.']\n",
      "In this\n",
      "chapter, we will learn about the various steps involved and how they play\n",
      "important roles in solving the NLP problem and we‚Äôll see a few guidelines\n",
      "about when and how to use which step.\n",
      "['In', 'this', 'chapter', ',', 'we', 'will', 'learn', 'about', 'the', 'various', 'steps', 'involved', 'and', 'how', 'they', 'play', 'important', 'roles', 'in', 'solving', 'the', 'NLP', 'problem', 'and', 'we', '‚Äô', 'll', 'see', 'a', 'few', 'guidelines', 'about', 'when', 'and', 'how', 'to', 'use', 'which', 'step', '.']\n",
      "In later chapters, we‚Äôll discuss\n",
      "specific pipelines for various NLP tasks (e.g., Chapters 4‚Äì7).r\n",
      "['In', 'later', 'chapters', ',', 'we', '‚Äô', 'll', 'discuss', 'specific', 'pipelines', 'for', 'various', 'NLP', 'tasks', '(', 'e.g.', ',', 'Chapters', '4‚Äì7', ')', '.r']\n"
     ]
    }
   ],
   "source": [
    "for sentence in my_sentences:\n",
    "    print(sentence)\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./Chapter%202%20NLP%20pipeline%20c140380e660240a9a622b98490b31b28/Untitled%203.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequent steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sau khi ƒë√£ sentence segmener v√† word tokenizer, ta c√≥ ƒë∆∞·ª£c m·∫£ng c√°c t·ª´ c√≥ nghƒ©a. Vi·ªác l√†m ti·∫øp theo ƒë√≥ l√† l·ªçc b·ªè ƒëi nh·ªØng t·ª´ kh√¥ng c·∫ßn thi·∫øt cho vi·ªác ph√¢n t√≠ch v√≠ d·ª• nh∆∞ c√°c t·ª´ `a, an, the, of, of, in...` hay c√≤n g·ªçi l√† c√°c `stop words`. Ngo√†i ra th√¨ c√≤n m·ªôt s·ªë t·ª´ ng·ªØ kh√°c n·ªØa kh√¥ng th·ª±c s·ª± li√™n quan ƒë·∫øn ng·ªØ c·∫£nh c≈©ng c·∫ßn ƒë∆∞·ª£c l·ªçc.\n",
    "- V·∫•n ƒë·ªÅ n·ªØa ƒë√≥ l√† v·ªÅ ch·ªØ hoa v√† ch·ªØ th∆∞·ªùng, th∆∞·ªùng th√¨ ta s·∫Ω ƒë·ªÉ hoa h·∫øt ho·∫∑c th∆∞·ªùng h·∫øt v√† h·∫ßu h·∫øt l√† ƒë·ªÉ th∆∞·ªùng h·∫øt.\n",
    "- Lo·∫°i b·ªè ƒëi c√°c d·∫•u c√¢u, m·ªôt s·ªë ch·ªØ s·ªë kh√¥ng c·∫ßn thi·∫øt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['in',\n",
       "  'previous',\n",
       "  'chapter',\n",
       "  'saw',\n",
       "  'examples',\n",
       "  'common',\n",
       "  'nlp',\n",
       "  'applications',\n",
       "  'might',\n",
       "  'encounter',\n",
       "  'everyday',\n",
       "  'life'],\n",
       " ['if',\n",
       "  'asked',\n",
       "  'build',\n",
       "  'application',\n",
       "  'think',\n",
       "  'would',\n",
       "  'approach',\n",
       "  'organization'],\n",
       " ['we',\n",
       "  'would',\n",
       "  'normally',\n",
       "  'walk',\n",
       "  'requirements',\n",
       "  'break',\n",
       "  'problem',\n",
       "  'several',\n",
       "  'sub-problems',\n",
       "  'try',\n",
       "  'develop',\n",
       "  'step-by-step',\n",
       "  'procedure',\n",
       "  'solve'],\n",
       " ['since',\n",
       "  'language',\n",
       "  'processing',\n",
       "  'involved',\n",
       "  'would',\n",
       "  'also',\n",
       "  'list',\n",
       "  'forms',\n",
       "  'text',\n",
       "  'processing',\n",
       "  'needed',\n",
       "  'step'],\n",
       " ['this', 'step-by-step', 'processing', 'text', 'known', 'pipeline'],\n",
       " ['it', 'series', 'steps', 'involved', 'building', 'nlp', 'model'],\n",
       " ['these',\n",
       "  'steps',\n",
       "  'common',\n",
       "  'every',\n",
       "  'nlp',\n",
       "  'project',\n",
       "  'makes',\n",
       "  'sense',\n",
       "  'study',\n",
       "  'chapter'],\n",
       " ['understanding',\n",
       "  'common',\n",
       "  'procedures',\n",
       "  'nlp',\n",
       "  'pipeline',\n",
       "  'enable',\n",
       "  'us',\n",
       "  'get',\n",
       "  'started',\n",
       "  'nlp',\n",
       "  'problem',\n",
       "  'encountered',\n",
       "  'workplace'],\n",
       " ['laying',\n",
       "  'developing',\n",
       "  'text-processing',\n",
       "  'pipeline',\n",
       "  'seen',\n",
       "  'starting',\n",
       "  'point',\n",
       "  'nlp',\n",
       "  'application',\n",
       "  'development',\n",
       "  'process'],\n",
       " ['in',\n",
       "  'chapter',\n",
       "  'learn',\n",
       "  'various',\n",
       "  'steps',\n",
       "  'involved',\n",
       "  'play',\n",
       "  'important',\n",
       "  'roles',\n",
       "  'solving',\n",
       "  'nlp',\n",
       "  'problem',\n",
       "  '‚Äô',\n",
       "  'see',\n",
       "  'guidelines',\n",
       "  'use',\n",
       "  'step'],\n",
       " ['in',\n",
       "  'later',\n",
       "  'chapters',\n",
       "  '‚Äô',\n",
       "  'discuss',\n",
       "  'specific',\n",
       "  'pipelines',\n",
       "  'various',\n",
       "  'nlp',\n",
       "  'tasks',\n",
       "  'e.g.',\n",
       "  'chapters',\n",
       "  '4‚Äì7',\n",
       "  '.r']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "punctuation = list(punctuation)\n",
    "\n",
    "def preprocess_corpus(texts):\n",
    "    mystopwords = set(stopwords.words(\"english\"))\n",
    "    def remove_stops_digits(tokens):\n",
    "        return [token.lower() for token in tokens if token not in mystopwords\n",
    "                        and not token.isdigit() and token not in punctuation]\n",
    "    return [remove_stops_digits(word_tokenize(text)) for text in texts]\n",
    "\n",
    "preprocess_corpus(my_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and lemmatization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming** c√≥ nghƒ©a l√† remove m·ªôt s·ªë ph·∫ßn prefix v√† suffix ƒëi ƒë·ªÉ ƒë∆∞·ª£c c√°c form gi·ªëng nhau v√≠ d·ª•:\n",
    "\n",
    "`'car' v√† 'cars'` s·∫Ω ƒë∆∞·ª£c stemming th√†nh `car`. Vi·ªác n√†y ƒë∆∞·ª£c th·ª±c hi·ªán b·∫±ng c√°ch apply m·ªôt s·ªë rules nh·∫•t ƒë·ªãnh v√≠ d·ª• nh∆∞ t·ª´ n√†o k·∫øt th√∫c b·∫±ng `es` th√¨ b·ªè ƒëi `es`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car car\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "word1, word2 = 'car', 'cars'\n",
    "\n",
    "\n",
    "print(stemmer.stem(word1), stemmer.stem(word2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ƒê·ªëi v·ªõi c√°c t·ª´ m√† bi·∫øn th·ªÉ c·ªßa n√≥ kh√¥ng ƒë∆°n gi·∫£n l√† th√™m suffix ho·∫∑c prefix v√≠ d·ª•: good, better,... m√† ch√∫ng v·∫´n c√≥ c√πng nghƒ©a v·ªõi nhau => c≈©ng c·∫ßn ph·∫£i chuy·ªÉn v·ªÅ d·∫°ng base form nh·∫•t ƒë·ªãnh. Vi·ªác th·ª±c hi·ªán n√†y ƒë∆∞·ª£c g·ªçi l√† `Lemmatization`.\n",
    "\n",
    "![](./Chapter%202%20NLP%20pipeline%20c140380e660240a9a622b98490b31b28/Untitled%204.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\hoang\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"better \", pos = \"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./Chapter%202%20NLP%20pipeline%20c140380e660240a9a622b98490b31b28/Untitled%205.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong ti·∫øng Vi·ªát th√¨ kh√¥ng c·∫ßn d√πng ƒë·∫øn stemming v√† lemmatization v√¨ ti·∫øng Vi·ªát kh√¥ng c√≥ ki·ªÉu bi·∫øn th·ªÉ nh∆∞ ti·∫øng Anh. 2 Kƒ© thu·∫≠t tr√™n th∆∞·ªùng ƒë∆∞·ª£c d√πng trong ti·∫øng Anh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Pre-processing Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text normalization\n",
    "\n",
    "#### Language Detection\n",
    "\n",
    "Khi x·ª≠ l√Ω text data v√≠ d·ª• nh∆∞ khi ƒëi crawl data t·ª´ m·ªôt trang web b√°n h√†ng n√†o ƒë√≥ th√¨ ta s·∫Ω g·∫∑p tr∆∞·ªùng h·ª£p ƒë√≥ l√† craw v·ªÅ nhi·ªÅu lo·∫°i ng√¥n ng·ªØ kh√°c nhau do ng∆∞·ªùi d√πng t·ª´ c√°c qu·ªëc gia kh√°c nhau b√¨nh lu·∫≠n, ... D√≥ ƒë√≥ vi·ªác ph√¢n lo·∫°i ra ng√¥n ng·ªØ ƒë·ªÉ x·ª≠ l√Ω l√† quan tr·ªçng. Ta c√≥ th·ªÉ s·ª≠ d·ª•ng th∆∞ vi·ªán Polygot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code mixing and transliteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Khi m·ªôt ng∆∞·ªùi bi·∫øt nhi·ªÅu ng√¥n ng·ªØ, c√≥ kh·∫£ nƒÉng trong khi n√≥i ho·∫∑c vi·∫øt, ng∆∞·ªùi ƒë√≥ s·∫Ω v√¥ √Ω d√πng multiple languages trong c√¢u => ƒê√¢y g·ªçi l√† `code mixing`.\n",
    "\n",
    "Trong khi vi·∫øt, khi ng∆∞·ªùi ƒë√≥ s·ª≠ d·ª•ng c√°c t·ª´ m√† m√¨nh n√≥i d∆∞·ªõi d·∫°ng ƒë√°nh v·∫ßn ng·ªØ √¢m trong ti·∫øng anh (v√≠ d·ª• `ch√†o` trong ti·∫øng vi·ªát n·∫øu n√≥i ng·ªØ √¢m theo ti·∫øng Anh s·∫Ω l√† `chao` => vi·∫øt th√†nh ch·ªØ l√† `chao`) => G·ªçi l√† transliteration.\n",
    "\n",
    "![](./Chapter%202%20NLP%20pipeline%20c140380e660240a9a622b98490b31b28/Untitled%206.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature engineering\n",
    "\n",
    "Sau khi qua ti·ªÅn x·ª≠ l√Ω raw text, b∆∞·ªõc ti·∫øp theo l√† bi·∫øn ƒë·ªïi data ƒë√£ qua x·ª≠ l√Ω v·ªÅ d·∫°ng data m√† c√°c model AI c√≥ th·ªÉ l√†m vi·ªác ƒë∆∞·ª£c, ƒë√≥ ch√≠nh l√† c√°c d·∫°ng ma tr·∫≠n v√† vector. Vi·ªác n√†y c√≤n ƒë∆∞·ª£c g·ªçi l√† `feature extraction`\n",
    "\n",
    "![](./Chapter%202%20NLP%20pipeline%20c140380e660240a9a622b98490b31b28/Untitled%207.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical NLP / ML Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DL Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T√≥m t·∫Øt l·∫°i ki·∫øn th·ª©c:\n",
    "\n",
    "- B√™n c·∫°nh vi·ªác d√πng Model ML v√† DL, vi·ªác k·∫øt h·ª£p s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p ph·ªèng ƒëo√°n (heuristic -> ki·ªÉu nh∆∞ rule-based, if else v·ªõi ƒëi·ªÅu ki·ªán n√†o ƒë√≥ cho d·ªØ li·ªáu). Vi·ªác s·ª≠ d·ª•ng c√°c ph∆∞∆°ng ph√°p t√πy thu·ªôc v√†o b√†i to√°n l·ªõn hay nh·ªè, d·ªØ li·ªáu nhi·ªÅu hay √≠t, kh√¥ng ph·∫£i l√∫c n√†o c≈©ng heuristic v√† kh√¥ng ph·∫£i l√∫c n√†o c≈©ng d√πng model.\n",
    "\n",
    "- Khi s·ª≠ d·ª•ng model th√¨ vi·ªác s·ª≠ d·ª•ng model stacking (S·ª≠ d·ª•ng nhi·ªÅu model trong ƒë√≥ ƒë·∫ßu ra model n√†y l√† ƒë·∫ßu v√†o cho model ti·∫øp theo) ho·∫∑c model ensemble (S·ª≠ d·ª•ng nhi·ªÅu model song song ƒë·ªÉ t√¨m ra model cu·ªëi c√πng) ƒë∆∞·ª£c khuy·∫øn kh√≠ch cho ra k·∫øt qu·∫£ t·ªët h∆°n -> Trong tr∆∞·ªùng h·ª£p ƒë√°p ·ª©ng ƒë∆∞·ª£c hi·ªáu nƒÉng y√™u c·∫ßu.\n",
    "\n",
    "Vi·ªác s·ª≠ d·ª•ng k·∫øt h·ª£p stacking + ensemble c√≥ th·ªÉ th·ªÉ hi·ªán qua h√¨nh sau:\n",
    "\n",
    "![](../assets/images/Screenshot%20from%202023-10-04%2009-59-50.png)\n",
    "\n",
    "- S·ª≠ d·ª•ng feature engineering m·ªôt c√°ch t·ªët h∆°n\n",
    "- S·ª≠ d·ª•ng transfer learning\n",
    "- S·ª≠ d·ª•ng heuristic ƒë·ªÉ handle m·ªôt s·ªë tr∆∞·ªùng h·ª£p nh·∫•t ƒë·ªãnh m√† model kh√¥ng th·ªÉ handle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi·ªác ph√°t tri·ªÉn b√†i to√°n d·ª±a theo data c√≥ th·ªÉ  tham kh·∫£o b·∫£ng sau:\n",
    "\n",
    "![](../assets/images/Screenshot%20from%202023-10-04%2010-03-19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation c√≥ 2 lo·∫°i:\n",
    "\n",
    "- Intrinsic: Focus v√†o intermediary objectives\n",
    "- Extrinsic: Focus v√†o final objectives\n",
    "\n",
    "\n",
    "V√≠ d·ª• v·ªõi h·ªá th·ªëng cls spam email:\n",
    "\n",
    "- Intrinsic: ƒëo ƒë·∫°c c√°c metric precision, recall\n",
    "- Extrinsic: ƒëo ƒë·∫°c th·ªùi gian m√† user wasted do vi·ªác sai s·ªë trong ph√¢n lo·∫°i email -> email r√°c v√†o inbox, email chu·∫©n v√†o th√πng r√°c\n",
    "\n",
    "Hay n√≥i c√°ch kh√°c Intrinsic ƒë√°nh gi√° model trong qu√° tr√¨nh ph√°t tri·ªÉn, c√≤n ƒë√°nh gi√° cu·ªëi c√πng khi model ƒëc ƒë∆∞a v√†o business th√¨ d√πng Extrinsic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intrinsic Evaluation\n",
    "\n",
    "![](../assets/images/Screenshot%20from%202023-10-04%2010-05-37.png)\n",
    "![](../assets/images/Screenshot%20from%202023-10-04%2010-05-58.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extrinsic Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Post-modeling phases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau khi c√≥ model th√¨ c·∫ßn:\n",
    "\n",
    "- Deploy model\n",
    "- Monitoring\n",
    "- Model updating\n",
    "\n",
    "![](../assets/images/Screenshot%20from%202023-10-04%2010-54-42.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Wokring with Other Languages\n",
    "\n",
    "![](../assets/images/Screenshot%20from%202023-10-04%2011-05-14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Case study\n",
    "\n",
    "Xem x√©t case studo v·ªÅ  tool c·∫£i thi·ªán chƒÉm s√≥c kh√°ch h√†ng c·ªßa Uber: Customer Obsession Ticketing Assistant (COTA)\n",
    "\n",
    "Uber v·∫≠n h√†nh h∆°n 400 th√†nh ph·ªë tr√™n to√†n th·∫ø gi·ªõi v√¨ v·∫≠y m√† l∆∞·ª£ng tickets cho c√°c v·∫•n ƒë·ªÅ kh√°c nhau h√†ng ng√†y l√† v√¥ c√πng nhi·ªÅu. V√† c≈©ng s·∫Ω c√≥ nhi·ªÅu solutions cho m·ªói ticket. M·ª•c ti√™u c·ªßa COTA l√† ranking c√°c solution n√†y v√† ch·ªçn ra c√°i t·ªët nh·∫•t cho ticket. \n",
    "\n",
    "![](../assets/images/Screenshot%20from%202023-10-04%2011-16-16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Break ra ch√∫t:\n",
    "\n",
    "- ƒê·∫ßu ti√™n th√¥ng tin c·∫ßn ƒë·ªÉ c√≥ th·ªÉ identify ticket issue v√† ch·ªçn ra solution ƒë·∫øn t·ª´ 3 ngu·ªìn:\n",
    "\n",
    "    - Ticket info (metadata)\n",
    "    - Ticket text -> N·ªôi dung c·ªßa ticket\n",
    "    - Trip data -> D·ªØ li·ªáu chuy·∫øn ƒëi c·ªßa kh√°ch h√†ng\n",
    "\n",
    "- Sau ƒë√≥ d·ªØ li·ªáu `Ticket text` ƒë∆∞·ª£c ƒë∆∞a qua pre-processing\n",
    "\n",
    "- Sau khi processing th√¨ d√πng LSI v√† TF-IDF ƒë·ªÉ extract feature. Process n√†y g·ªçi l√† topic modeling:\n",
    "\n",
    "    - Chi ti·∫øt v·ªÅ c√°ch Uber d√πng task n√†y l√†: Uber thu th·∫≠p l·ªãch s·ª≠ c√°c tickets cho m·ªói solution t·ª´ CSDL, t·∫°o ra bag-of-words vector representation cho m·ªói solution v√† t·∫°o topic model d·ª±a tr√™n c√°c representation n√†y. Khi ƒë√≥ v·ªõi ticket input cho model, n√≥ s·∫Ω ƒë∆∞·ª£c t√≠nh to√°n cosine similarity v·ªõi m·ªói solution -> Nh·∫≠n ƒë∆∞·ª£c k·∫øt qu·∫£ l√† ticker text's similarity cho t·∫•t c·∫£ c√°c possible solutions.\n",
    "    - B∆∞·ªõc ti·∫øp theo l√† k·∫øt h·ª£p k·∫øt qu·∫£ cosine similarity t·ª´ b∆∞·ªõc tr∆∞·ªõc v·ªõi Ticket info v√† Trip data ƒë·ªÉ ranking, ch·ªçn ra 3 solution t·ªët nh·∫•t."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9d2e2287b90f84c05b564773cad156a65fe051f83fa7c81a3ad23c3ed1bf9926"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
