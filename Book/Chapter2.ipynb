{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2. NLP pipeline\n",
    "\n",
    "The key stages in the NLP pipeline:\n",
    "\n",
    "1. Data acquisition\n",
    "2. Text cleaning\n",
    "3. Pre-processing\n",
    "4. Feature engineering\n",
    "5. Modeling\n",
    "6. Evaluation\n",
    "7. Deployment\n",
    "8. Monitoring and model updating\n",
    "\n",
    "![Untitled](Chapter%202%20NLP%20pipeline%20c140380e660240a9a622b98490b31b28/Untitled.png)\n",
    "\n",
    "## 1. Data Acquisition\n",
    "\n",
    "Ways to collecting data:\n",
    "\n",
    "- Use a public dataset\n",
    "- Scrape data: crawl data t·ª´ internet\n",
    "- Product intervention: Thu th·∫≠p data t·ª´ vi·ªác ch·∫°y c√°c s·∫£n ph·∫©m th·ª±c t·∫ø\n",
    "- Synonym replacement: Ch·ªçn ng·∫´u nhi√™n `k` t·ª´ trong c√¢u m√† kh√¥ng ph·∫£i l√† t·ª´ k·∫øt th√∫c c√¢u, replace c√°c t·ª´ n√†y b·∫±ng synonyms c·ªßa ch√∫ng. Ta c√≥ th·ªÉ d√πng Synsets trong Wordnet [3,4]\n",
    "- Back translation: Gi·∫£ s·ª≠ ta c√≥ S1 b·∫±ng ti·∫øng Anh, ta c√≥ th·ªÉ d√πng google translate ƒë·ªÉ d·ªãch n√≥ sang S2 l√† ti·∫øng ƒê·ª©c, t·ª´ S2 ta l·∫°i d√πng n√≥ ƒë·ªÉ d·ªãch ng∆∞·ª£c l·∫°i ti·∫øng Anh ‚áí 2 c√¢u c√≥ ng·ªØ nghƒ©a l√† gi·ªëng nhau nh∆∞ng c·∫•u tr√∫c c·ªßa n√≥ l·∫°i kh√°c.\n",
    "\n",
    "![Untitled](Chapter%202%20NLP%20pipeline%20c140380e660240a9a622b98490b31b28/Untitled%201.png)\n",
    "\n",
    "- TF-IDF-based word replacement\n",
    "- Bigram flipping\n",
    "- Replacing entities\n",
    "- Adding noise to data\n",
    "- Advanced techniques:\n",
    "    - Snorkel\n",
    "    - Easy Data Augmentation\n",
    "    - Active Learning\n",
    "\n",
    "## 2. Text extraction and cleanup\n",
    "\n",
    "![Untitled](Chapter%202%20NLP%20pipeline%20c140380e660240a9a622b98490b31b28/Untitled%202.png)\n",
    "\n",
    "### HTML Parsing and Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unicode Normalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'I love \\xf0\\x9f\\x8d\\x95! Shall we book a \\xf0\\x9f\\x9a\\x97 to gizza?'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import emoji\n",
    "\n",
    "text = 'I love üçï! Shall we book a üöó to gizza?'\n",
    "Text = text.encode(\"utf-8\")\n",
    "\n",
    "Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spelling Correction\n",
    "\n",
    "C√≥ th·ªÉ s·ª≠ d·ª•ng REST API cho vi·ªác s·ª≠a l·ªói ch√≠nh t·∫£ c·ªßa Microsoft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"error\": {\n",
      "        \"code\": \"401\",\n",
      "        \"message\": \"Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "api_key = \"\"\n",
    "example_text = \"Hollo, wrld\"\n",
    "endpoint = \"https://api.cognitive.microsoft.com/bing/v7.0/SpellCheck\"\n",
    "\n",
    "data = {\n",
    "    'text': example_text\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'mkt':'en-us',\n",
    "    'mode':'proof'\n",
    "    }\n",
    "\n",
    "headers = {\n",
    "    'Content-Type': 'application/x-www-form-urlencoded',\n",
    "    'Ocp-Apim-Subscription-Key': api_key,\n",
    "    }\n",
    "\n",
    "response = requests.post(endpoint, headers=headers, params=params, data=data)\n",
    "\n",
    "json_response = response.json()\n",
    "print(json.dumps(json_response, indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System-Specific Error Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the nineteenth century the only kind of linguistics considered\n",
      "seriously was this comparative and historical stady of word in angus\n",
      "\n",
      "known or believed to be cagnate‚Äîsay the Semitic languages, or the Indo-\n",
      "European languages. It is significant that the Germans who really made\n",
      "the subject what it was, used the term Indo-germanisch, Those who know\n",
      "the popular works of Otto Jespersen will remember how firmly he\n",
      "declares that linguistic science is historical. And those who have noticed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from pytesseract import image_to_string\n",
    "import pytesseract\n",
    "\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd  = r'C:\\Program Files\\Tesseract-OCR\\tesseract'\n",
    "filename = \"./test_img.png\"\n",
    "\n",
    "text = image_to_string(Image.open(filename))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "## 3. Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Segmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nIn the previous chapter, we saw examples of some common NLP\\napplications that we might encounter in everyday life.',\n",
       " 'If we were asked to\\nbuild such an application, think about how we would approach doing so at our\\norganization.',\n",
       " 'We would normally walk through the requirements and break the\\nproblem down into several sub-problems, then try to develop a step-by-step\\nprocedure to solve them.',\n",
       " 'Since language processing is involved, we would also\\nlist all the forms of text processing needed at each step.',\n",
       " 'This step-by-step\\nprocessing of text is known as pipeline.',\n",
       " 'It is the series of steps involved in\\nbuilding any NLP model.',\n",
       " 'These steps are common in every NLP project, so it\\nmakes sense to study them in this chapter.',\n",
       " 'Understanding some common procedures\\nin any NLP pipeline will enable us to get started on any NLP problem encountered\\nin the workplace.',\n",
       " 'Laying out and developing a text-processing pipeline is seen\\nas a starting point for any NLP application development process.',\n",
       " 'In this\\nchapter, we will learn about the various steps involved and how they play\\nimportant roles in solving the NLP problem and we‚Äôll see a few guidelines\\nabout when and how to use which step.',\n",
       " 'In later chapters, we‚Äôll discuss\\nspecific pipelines for various NLP tasks (e.g., Chapters 4‚Äì7).r']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "mytext = \"\"\"\n",
    "In the previous chapter, we saw examples of some common NLP\n",
    "applications that we might encounter in everyday life. If we were asked to\n",
    "build such an application, think about how we would approach doing so at our\n",
    "organization. We would normally walk through the requirements and break the\n",
    "problem down into several sub-problems, then try to develop a step-by-step\n",
    "procedure to solve them. Since language processing is involved, we would also\n",
    "list all the forms of text processing needed at each step. This step-by-step\n",
    "processing of text is known as pipeline. It is the series of steps involved in\n",
    "building any NLP model. These steps are common in every NLP project, so it\n",
    "makes sense to study them in this chapter. Understanding some common procedures\n",
    "in any NLP pipeline will enable us to get started on any NLP problem encountered\n",
    "in the workplace. Laying out and developing a text-processing pipeline is seen\n",
    "as a starting point for any NLP application development process. In this\n",
    "chapter, we will learn about the various steps involved and how they play\n",
    "important roles in solving the NLP problem and we‚Äôll see a few guidelines\n",
    "about when and how to use which step. In later chapters, we‚Äôll discuss\n",
    "specific pipelines for various NLP tasks (e.g., Chapters 4‚Äì7).r\n",
    "\"\"\"\n",
    "\n",
    "my_sentences = sent_tokenize(mytext)\n",
    "\n",
    "my_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In the previous chapter, we saw examples of some common NLP\n",
      "applications that we might encounter in everyday life.\n",
      "['In', 'the', 'previous', 'chapter', ',', 'we', 'saw', 'examples', 'of', 'some', 'common', 'NLP', 'applications', 'that', 'we', 'might', 'encounter', 'in', 'everyday', 'life', '.']\n",
      "If we were asked to\n",
      "build such an application, think about how we would approach doing so at our\n",
      "organization.\n",
      "['If', 'we', 'were', 'asked', 'to', 'build', 'such', 'an', 'application', ',', 'think', 'about', 'how', 'we', 'would', 'approach', 'doing', 'so', 'at', 'our', 'organization', '.']\n",
      "We would normally walk through the requirements and break the\n",
      "problem down into several sub-problems, then try to develop a step-by-step\n",
      "procedure to solve them.\n",
      "['We', 'would', 'normally', 'walk', 'through', 'the', 'requirements', 'and', 'break', 'the', 'problem', 'down', 'into', 'several', 'sub-problems', ',', 'then', 'try', 'to', 'develop', 'a', 'step-by-step', 'procedure', 'to', 'solve', 'them', '.']\n",
      "Since language processing is involved, we would also\n",
      "list all the forms of text processing needed at each step.\n",
      "['Since', 'language', 'processing', 'is', 'involved', ',', 'we', 'would', 'also', 'list', 'all', 'the', 'forms', 'of', 'text', 'processing', 'needed', 'at', 'each', 'step', '.']\n",
      "This step-by-step\n",
      "processing of text is known as pipeline.\n",
      "['This', 'step-by-step', 'processing', 'of', 'text', 'is', 'known', 'as', 'pipeline', '.']\n",
      "It is the series of steps involved in\n",
      "building any NLP model.\n",
      "['It', 'is', 'the', 'series', 'of', 'steps', 'involved', 'in', 'building', 'any', 'NLP', 'model', '.']\n",
      "These steps are common in every NLP project, so it\n",
      "makes sense to study them in this chapter.\n",
      "['These', 'steps', 'are', 'common', 'in', 'every', 'NLP', 'project', ',', 'so', 'it', 'makes', 'sense', 'to', 'study', 'them', 'in', 'this', 'chapter', '.']\n",
      "Understanding some common procedures\n",
      "in any NLP pipeline will enable us to get started on any NLP problem encountered\n",
      "in the workplace.\n",
      "['Understanding', 'some', 'common', 'procedures', 'in', 'any', 'NLP', 'pipeline', 'will', 'enable', 'us', 'to', 'get', 'started', 'on', 'any', 'NLP', 'problem', 'encountered', 'in', 'the', 'workplace', '.']\n",
      "Laying out and developing a text-processing pipeline is seen\n",
      "as a starting point for any NLP application development process.\n",
      "['Laying', 'out', 'and', 'developing', 'a', 'text-processing', 'pipeline', 'is', 'seen', 'as', 'a', 'starting', 'point', 'for', 'any', 'NLP', 'application', 'development', 'process', '.']\n",
      "In this\n",
      "chapter, we will learn about the various steps involved and how they play\n",
      "important roles in solving the NLP problem and we‚Äôll see a few guidelines\n",
      "about when and how to use which step.\n",
      "['In', 'this', 'chapter', ',', 'we', 'will', 'learn', 'about', 'the', 'various', 'steps', 'involved', 'and', 'how', 'they', 'play', 'important', 'roles', 'in', 'solving', 'the', 'NLP', 'problem', 'and', 'we', '‚Äô', 'll', 'see', 'a', 'few', 'guidelines', 'about', 'when', 'and', 'how', 'to', 'use', 'which', 'step', '.']\n",
      "In later chapters, we‚Äôll discuss\n",
      "specific pipelines for various NLP tasks (e.g., Chapters 4‚Äì7).r\n",
      "['In', 'later', 'chapters', ',', 'we', '‚Äô', 'll', 'discuss', 'specific', 'pipelines', 'for', 'various', 'NLP', 'tasks', '(', 'e.g.', ',', 'Chapters', '4‚Äì7', ')', '.r']\n"
     ]
    }
   ],
   "source": [
    "for sentence in my_sentences:\n",
    "    print(sentence)\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./Chapter%202%20NLP%20pipeline%20c140380e660240a9a622b98490b31b28/Untitled%203.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequent steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sau khi ƒë√£ sentence segmener v√† word tokenizer, ta c√≥ ƒë∆∞·ª£c m·∫£ng c√°c t·ª´ c√≥ nghƒ©a. Vi·ªác l√†m ti·∫øp theo ƒë√≥ l√† l·ªçc b·ªè ƒëi nh·ªØng t·ª´ kh√¥ng c·∫ßn thi·∫øt cho vi·ªác ph√¢n t√≠ch v√≠ d·ª• nh∆∞ c√°c t·ª´ `a, an, the, of, of, in...` hay c√≤n g·ªçi l√† c√°c `stop words`. Ngo√†i ra th√¨ c√≤n m·ªôt s·ªë t·ª´ ng·ªØ kh√°c n·ªØa kh√¥ng th·ª±c s·ª± li√™n quan ƒë·∫øn ng·ªØ c·∫£nh c≈©ng c·∫ßn ƒë∆∞·ª£c l·ªçc.\n",
    "- V·∫•n ƒë·ªÅ n·ªØa ƒë√≥ l√† v·ªÅ ch·ªØ hoa v√† ch·ªØ th∆∞·ªùng, th∆∞·ªùng th√¨ ta s·∫Ω ƒë·ªÉ hoa h·∫øt ho·∫∑c th∆∞·ªùng h·∫øt v√† h·∫ßu h·∫øt l√† ƒë·ªÉ th∆∞·ªùng h·∫øt.\n",
    "- Lo·∫°i b·ªè ƒëi c√°c d·∫•u c√¢u, m·ªôt s·ªë ch·ªØ s·ªë kh√¥ng c·∫ßn thi·∫øt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['in',\n",
       "  'previous',\n",
       "  'chapter',\n",
       "  'saw',\n",
       "  'examples',\n",
       "  'common',\n",
       "  'nlp',\n",
       "  'applications',\n",
       "  'might',\n",
       "  'encounter',\n",
       "  'everyday',\n",
       "  'life'],\n",
       " ['if',\n",
       "  'asked',\n",
       "  'build',\n",
       "  'application',\n",
       "  'think',\n",
       "  'would',\n",
       "  'approach',\n",
       "  'organization'],\n",
       " ['we',\n",
       "  'would',\n",
       "  'normally',\n",
       "  'walk',\n",
       "  'requirements',\n",
       "  'break',\n",
       "  'problem',\n",
       "  'several',\n",
       "  'sub-problems',\n",
       "  'try',\n",
       "  'develop',\n",
       "  'step-by-step',\n",
       "  'procedure',\n",
       "  'solve'],\n",
       " ['since',\n",
       "  'language',\n",
       "  'processing',\n",
       "  'involved',\n",
       "  'would',\n",
       "  'also',\n",
       "  'list',\n",
       "  'forms',\n",
       "  'text',\n",
       "  'processing',\n",
       "  'needed',\n",
       "  'step'],\n",
       " ['this', 'step-by-step', 'processing', 'text', 'known', 'pipeline'],\n",
       " ['it', 'series', 'steps', 'involved', 'building', 'nlp', 'model'],\n",
       " ['these',\n",
       "  'steps',\n",
       "  'common',\n",
       "  'every',\n",
       "  'nlp',\n",
       "  'project',\n",
       "  'makes',\n",
       "  'sense',\n",
       "  'study',\n",
       "  'chapter'],\n",
       " ['understanding',\n",
       "  'common',\n",
       "  'procedures',\n",
       "  'nlp',\n",
       "  'pipeline',\n",
       "  'enable',\n",
       "  'us',\n",
       "  'get',\n",
       "  'started',\n",
       "  'nlp',\n",
       "  'problem',\n",
       "  'encountered',\n",
       "  'workplace'],\n",
       " ['laying',\n",
       "  'developing',\n",
       "  'text-processing',\n",
       "  'pipeline',\n",
       "  'seen',\n",
       "  'starting',\n",
       "  'point',\n",
       "  'nlp',\n",
       "  'application',\n",
       "  'development',\n",
       "  'process'],\n",
       " ['in',\n",
       "  'chapter',\n",
       "  'learn',\n",
       "  'various',\n",
       "  'steps',\n",
       "  'involved',\n",
       "  'play',\n",
       "  'important',\n",
       "  'roles',\n",
       "  'solving',\n",
       "  'nlp',\n",
       "  'problem',\n",
       "  '‚Äô',\n",
       "  'see',\n",
       "  'guidelines',\n",
       "  'use',\n",
       "  'step'],\n",
       " ['in',\n",
       "  'later',\n",
       "  'chapters',\n",
       "  '‚Äô',\n",
       "  'discuss',\n",
       "  'specific',\n",
       "  'pipelines',\n",
       "  'various',\n",
       "  'nlp',\n",
       "  'tasks',\n",
       "  'e.g.',\n",
       "  'chapters',\n",
       "  '4‚Äì7',\n",
       "  '.r']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "punctuation = list(punctuation)\n",
    "\n",
    "def preprocess_corpus(texts):\n",
    "    mystopwords = set(stopwords.words(\"english\"))\n",
    "    def remove_stops_digits(tokens):\n",
    "        return [token.lower() for token in tokens if token not in mystopwords\n",
    "                        and not token.isdigit() and token not in punctuation]\n",
    "    return [remove_stops_digits(word_tokenize(text)) for text in texts]\n",
    "\n",
    "preprocess_corpus(my_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and lemmatization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming** c√≥ nghƒ©a l√† remove m·ªôt s·ªë ph·∫ßn prefix v√† suffix ƒëi ƒë·ªÉ ƒë∆∞·ª£c c√°c form gi·ªëng nhau v√≠ d·ª•:\n",
    "\n",
    "`'car' v√† 'cars'` s·∫Ω ƒë∆∞·ª£c stemming th√†nh `car`. Vi·ªác n√†y ƒë∆∞·ª£c th·ª±c hi·ªán b·∫±ng c√°ch apply m·ªôt s·ªë rules nh·∫•t ƒë·ªãnh v√≠ d·ª• nh∆∞ t·ª´ n√†o k·∫øt th√∫c b·∫±ng `es` th√¨ b·ªè ƒëi `es`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car car\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "word1, word2 = 'car', 'cars'\n",
    "\n",
    "\n",
    "print(stemmer.stem(word1), stemmer.stem(word2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ƒê·ªëi v·ªõi c√°c t·ª´ m√† bi·∫øn th·ªÉ c·ªßa n√≥ kh√¥ng ƒë∆°n gi·∫£n l√† th√™m suffix ho·∫∑c prefix v√≠ d·ª•: good, better,... m√† ch√∫ng v·∫´n c√≥ c√πng nghƒ©a v·ªõi nhau => c≈©ng c·∫ßn ph·∫£i chuy·ªÉn v·ªÅ d·∫°ng base form nh·∫•t ƒë·ªãnh. Vi·ªác th·ª±c hi·ªán n√†y ƒë∆∞·ª£c g·ªçi l√† `Lemmatization`.\n",
    "\n",
    "![](./Chapter%202%20NLP%20pipeline%20c140380e660240a9a622b98490b31b28/Untitled%204.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\hoang\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"better \", pos = \"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./Chapter%202%20NLP%20pipeline%20c140380e660240a9a622b98490b31b28/Untitled%205.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Pre-processing Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text normalization\n",
    "\n",
    "#### Language Detection\n",
    "\n",
    "Khi x·ª≠ l√Ω text data v√≠ d·ª• nh∆∞ khi ƒëi crawl data t·ª´ m·ªôt trang web b√°n h√†ng n√†o ƒë√≥ th√¨ ta s·∫Ω g·∫∑p tr∆∞·ªùng h·ª£p ƒë√≥ l√† craw v·ªÅ nhi·ªÅu lo·∫°i ng√¥n ng·ªØ kh√°c nhau do ng∆∞·ªùi d√πng t·ª´ c√°c qu·ªëc gia kh√°c nhau b√¨nh lu·∫≠n, ... D√≥ ƒë√≥ vi·ªác ph√¢n lo·∫°i ra ng√¥n ng·ªØ ƒë·ªÉ x·ª≠ l√Ω l√† quan tr·ªçng. Ta c√≥ th·ªÉ s·ª≠ d·ª•ng th∆∞ vi·ªán Polygot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code mixing and transliteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Khi m·ªôt ng∆∞·ªùi bi·∫øt nhi·ªÅu ng√¥n ng·ªØ, c√≥ kh·∫£ nƒÉng trong khi n√≥i ho·∫∑c vi·∫øt, ng∆∞·ªùi ƒë√≥ s·∫Ω v√¥ √Ω d√πng multiple languages trong c√¢u => ƒê√¢y g·ªçi l√† `code mixing`.\n",
    "\n",
    "Trong khi vi·∫øt, khi ng∆∞·ªùi ƒë√≥ s·ª≠ d·ª•ng c√°c t·ª´ m√† m√¨nh n√≥i d∆∞·ªõi d·∫°ng ƒë√°nh v·∫ßn ng·ªØ √¢m trong ti·∫øng anh (v√≠ d·ª• `ch√†o` trong ti·∫øng vi·ªát n·∫øu n√≥i ng·ªØ √¢m theo ti·∫øng Anh s·∫Ω l√† `chao` => vi·∫øt th√†nh ch·ªØ l√† `chao`) => G·ªçi l√† transliteration.\n",
    "\n",
    "![](./Chapter%202%20NLP%20pipeline%20c140380e660240a9a622b98490b31b28/Untitled%206.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature engineering\n",
    "\n",
    "Sau khi qua ti·ªÅn x·ª≠ l√Ω raw text, b∆∞·ªõc ti·∫øp theo l√† bi·∫øn ƒë·ªïi data ƒë√£ qua x·ª≠ l√Ω v·ªÅ d·∫°ng data m√† c√°c model AI c√≥ th·ªÉ l√†m vi·ªác ƒë∆∞·ª£c, ƒë√≥ ch√≠nh l√† c√°c d·∫°ng ma tr·∫≠n v√† vector. Vi·ªác n√†y c√≤n ƒë∆∞·ª£c g·ªçi l√† `feature extraction`\n",
    "\n",
    "![](./Chapter%202%20NLP%20pipeline%20c140380e660240a9a622b98490b31b28/Untitled%207.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical NLP / ML Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DL Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modeling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9d2e2287b90f84c05b564773cad156a65fe051f83fa7c81a3ad23c3ed1bf9926"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
