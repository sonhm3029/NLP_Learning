{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 - Text Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../assets/images/Screenshot%20from%202023-10-04%2014-45-21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text representation được phân làm 4 loại:\n",
    "- Basic vectorization approaches\n",
    "- Distributed representations\n",
    "- Universal language representation\n",
    "- Handcrafted features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vector space models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tính toán độ tương đồng giữa 2 vector:\n",
    "\n",
    "Cosine\n",
    "\n",
    "![](../assets/images/Screenshot%20from%202023-10-04%2014-54-46.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ngoài ra còn dùng khoảng cách Euclid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Vectorization Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bắt đầu với ý tưởng đơn giản về text representation: map mỗi từ trong vocabulary (V) của text corpus tới một unique ID (integer value), \n",
    "Khi đó mỗi câu sẽ được biểu diễn bằng một vector `V` chiều.\n",
    "\n",
    "Ví dụ có 4 văn bản:\n",
    "\n",
    "![](../assets/images/Screenshot%20from%202023-10-04%2015-00-27.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau khi apply các phương pháp pre-processing vào ta được vocabulary: [dogs, bites, man, eats, meat, food] -> 6 từ. Như vậy với mỗi Documents bây giờ ta có thể represented bằng một vector có số chiều là 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ví dụ đối với vocab [dogs, bites, man, eats, meat, food]:\n",
    "\n",
    "- dogs  -> [1, 0, 0, 0, 0, 0]\n",
    "- bites -> [0, 1, 0, 0, 0, 0]\n",
    "- man   -> [0, 0, 1, 0, 0, 0]\n",
    "- eats  -> [0, 0, 0, 1, 0, 0]\n",
    "- meat  -> [0, 0, 0, 0, 1, 0]\n",
    "- food  -> [0, 0, 0, 0, 0, 1]\n",
    "\n",
    "\n",
    "Như vậy với câu \"Dog bites man\" được biểu diễn:\n",
    "[\n",
    "    [1, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 0]\n",
    "]\n",
    "\n",
    "Với kiểu mã hóa như thế này thì có lợi là nó khá dễ hiểu và dễ dàng thực hiện tuy nhiên gặp một số mặt hại như sau:\n",
    "\n",
    "- Dữ liệu biểu diễn có quá nhiều giá trị 0 trong điều kiện thực thế với kích thước vocab lớn\n",
    "- Không có fix length cho mỗi câu do các câu có số lượng từ khác nhau. Ta cần feature có length như nhau\n",
    "- Out of vocabulary (OOV) khi triển khai model gặp phải câu có từ không có trong từ điển -> Phải train lại"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viết tắt là BoW, là một kĩ thuật text representation cổ điển được dùng khá phổ biến trong NLP đặc biệt trong text classification problems. Với cách biểu diễn này, ta bỏ qua mối quan hệ về thứ tự và ngữ cảnh trong câu.\n",
    "\n",
    "Giống như One-Hot Encoding về việc dùng Vocabulary vector\n",
    "\n",
    "Ví dụ với câu \"Dog bites man\" được biểu diễn là [1, 1, 1, 0, 0, 0]. Hiểu như sau với vector vocab: [dogs, bites, man, eats, meat, food]. Mỗi câu sẽ được biểu diễn bởi vector 6 chiều. Trong đó giá trị của mỗi phần từ là số lần xúât hiện của từ với index tương ứng trong vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"] #Same as the earlier notebook\n",
    "processed_docs = [doc.lower().replace(\".\",\"\") for doc in documents]\n",
    "processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our corpus:  ['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']\n",
      "Our vocabulary:  {'dog': 1, 'bites': 0, 'man': 4, 'eats': 2, 'meat': 5, 'food': 3}\n",
      "BoW representation for 'dog bites man':  [[1 1 0 0 1 0]]\n",
      "BoW representation for 'man bites dog:  [[1 1 0 0 1 0]]\n",
      "Bow representation for 'dog and dog are friends': [[0 2 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#look at the documents list\n",
    "print(\"Our corpus: \", processed_docs)\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "#Build a BOW representation for the corpus\n",
    "bow_rep = count_vect.fit_transform(processed_docs)\n",
    "\n",
    "#Look at the vocabulary mapping\n",
    "print(\"Our vocabulary: \", count_vect.vocabulary_)\n",
    "\n",
    "#see the BOW rep for first 2 documents\n",
    "print(\"BoW representation for 'dog bites man': \", bow_rep[0].toarray())\n",
    "print(\"BoW representation for 'man bites dog: \",bow_rep[1].toarray())\n",
    "\n",
    "#Get the representation using this vocabulary, for a new text\n",
    "temp = count_vect.transform([\"dog and dog are friends\"])\n",
    "print(\"Bow representation for 'dog and dog are friends':\", temp.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ví dụ trong đoạn code trên ta thấy câu 'dog and dog are friends' có giá trị là 2 tại phần tử tương ứng với từ dog, đây là trường hợp ta quan tâm đến số lần xuát hiện của từ. Trong một số trường hợp ta không quan tâm đến việc này mà chỉ quan tâm là từ đó có xuất hiện trong câu hay không, khi đó giá trị biểu diễn của vector chỉ là 0 hoặc 1, không hoặc có xuất hiện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bow representation for 'dog and dog are friends': [[0 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#BoW with binary vectors\n",
    "count_vect = CountVectorizer(binary=True)\n",
    "count_vect.fit(processed_docs)\n",
    "temp = count_vect.transform([\"dog and dog are friends\"])\n",
    "print(\"Bow representation for 'dog and dog are friends':\", temp.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Với cách biểu diễn bằng BoW thì các câu đã được biểu diễn bởi các vector có độ dài như nhau, dễ dàng tính toán khoảng cách giữa 2 vector hơn. Tuy nhiên cách biểu diễn này có các hạn chế:\n",
    "\n",
    "- Size của vector tăng lên khi size của vocabulary tăng. Sparsity (Có quá nhiều phần tử giá trị bằng 0) vẫn là một vấn đề. Có thể hạn chế vấn đề này bằng cách hạn chế đi số lượng vocabulary thành các từ hay gặp nhất\n",
    "- Do không quan tâm về thứ tự nên 2 câu D1, D2  mặc dù là khác nghĩa nhau nhưng sẽ như nhau nếu sd BoW\n",
    "- Vẫn không cải thiện đc vấn đề về từ mới xuất hiện trong thực tế không có trong từ điển"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Bag of N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 cách biểu diễn khảo sát ở trên treat các từ như các units độc lập, không liên quan ngữ nghĩa gì với nhau, cũng không hề quan tâm đến thứ tự. Phương pháp tiếp cận sử dụng bag of n grams (BoN) ra đời để khắc phục điều này.\n",
    "\n",
    "Phương pháp BoN cũng break câu ra thành các phần gồm n từ liên tiếp (hay còn gọi là tokens, khác với pp trc mỗi tokens chỉ là 1 từ độc lập). Cách này giúp ta nắm bắt được 1 số ngữa cảnh. Mỗi đoạn (chunk) được gọi là `n-gram`. Như vậy corpus vocabulary, V, bây giờ là một tập hợp của các n-grams khác nhau. Sau đó mỗi document trong corpus được biểu diễn bởi một vector `V` chiều. Vector này cấu thành bởi các giá trị là số lần xuất hiện của mỗi n-gram xuất hiện trong document và giá trị 0 nếu n-gram không xuất hiện trong document.\n",
    "\n",
    "Xét các ví dụ documents cũ ở trên ta có thể dựng được tập các 2-gram (bigrams) là {dog bites, bites man, man bites, bites dog, dog eats, eats meat, man eats, eats food}. Như vậy biểu diễn cho các documents sẽ là vector 8 chiều:\n",
    "\n",
    "Với\n",
    "\n",
    "D1: [1, 1, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "D2: [0, 0, 1, 1, 0, 0, 0, 0]\n",
    "\n",
    "Với phương pháp dùng n-gram này khi n=1 thì = BoW hay unigram, n=2 được gọi là bigram, n=3 đc gọi là trigram\n",
    "\n",
    "Phương pháp này cũng đc gọi là `n-gram feature selection`\n",
    "\n",
    "Ta có thể sử dụng tăng dần `n` trong `n-gram` để có thể bắt được nhiều ngữ cảnh của câu hơn.\n",
    "\n",
    "Ví dụ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#our corpus\n",
    "documents = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"]\n",
    "\n",
    "processed_docs = [doc.lower().replace(\".\",\"\") for doc in documents]\n",
    "processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our vocabulary:  {'dog': 3, 'bites': 0, 'man': 12, 'dog bites': 4, 'bites man': 2, 'dog bites man': 5, 'man bites': 13, 'bites dog': 1, 'man bites dog': 14, 'eats': 8, 'meat': 17, 'dog eats': 6, 'eats meat': 10, 'dog eats meat': 7, 'food': 11, 'man eats': 15, 'eats food': 9, 'man eats food': 16}\n",
      "BoW representation for 'dog bites man':  [[1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0]]\n",
      "BoW representation for 'man bites dog:  [[1 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0]]\n",
      "Bow representation for 'dog and dog are friends': [[0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Ngram vectorization example with count vectorizer and uni, bi, trigrams\n",
    "count_vect = CountVectorizer(ngram_range=(1,3))\n",
    "\n",
    "#Build a BOW representation for the corpus\n",
    "bow_rep = count_vect.fit_transform(processed_docs)\n",
    "\n",
    "#Look at the vocabulary mapping\n",
    "print(\"Our vocabulary: \", count_vect.vocabulary_)\n",
    "\n",
    "#see the BOW rep for first 2 documents\n",
    "print(\"BoW representation for 'dog bites man': \", bow_rep[0].toarray())\n",
    "print(\"BoW representation for 'man bites dog: \",bow_rep[1].toarray())\n",
    "\n",
    "#Get the representation using this vocabulary, for a new text\n",
    "temp = count_vect.transform([\"dog and dog are friends\"])\n",
    "\n",
    "print(\"Bow representation for 'dog and dog are friends':\", temp.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ở ví dụ trên ta dùng ngram_range để thu được vocab là kết hợp của 1-gram, 2-gram và 3-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Một số mặt lợi và hạn chế của BoN:\n",
    "\n",
    "- Bắt được ngữ cảnh và trật tự từ\n",
    "- ngram càng tăng thì kích thước vector và vấn đề về sparsity tăng lên rất nhanh\n",
    "- Vẫn chưa giải quyết được vấn đè OOV ( Gặp từ không có trong từ điển)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong 3 cách biểu diễn đưuọc nếu trc: One-hot, BoW và BoN thì các từ trong câu được treat quan trọng như nhau, không hề có sự lưu ý, chú ý đến từ nào quan trọng hơn. Phương pháp TF-IDF (term frequency-inverse document frequency) sinh ra để giải quyết vấn đề này. TF-IDF nhằm mục đích định lượng tầm quan trọng của một từ nhất định so với các từ khác trong tài liệu và trong kho văn bản (corpus). Phương pháp này thường được sử dụng trong information-retrieval systems để tìm ra các document từ kho văn bản (corpus) với query cho bởi user.\n",
    "\n",
    "TF-IDF thực hiện như sau: Nếu một từ `w` xuất hiện nhiều lần trong một văn bản $d_i$ nhưng hầu như không xuất hiện trong văn bản $d_j$. Vậy từ `w` sẽ rất quan trọng đối với văn bản $d_i$ nhưng lại không quan trọng lắm với $d_j$.\n",
    "\n",
    "TF-IDF đưuọc cấu thành vởi TF và IDF\n",
    "\n",
    "`TF` (term frequency) để đo đếm tấn suất lần xuất hiện của một từ trong một văn bản. Tuy nhiên do các văn bản có độ dài khác nhau nên có thể trong văn bản dài hơn, các từ có tấn suất xuất hiện nhiều hơn so với các văn bản ngắn hơn. Vì vậy cần phải normalize cho tổng số từ có trong văn bản. Ta có công thức\n",
    "\n",
    "\n",
    "$$TF(t,d) = \\frac{Number\\;of\\;occurrences\\;of\\;term\\;t\\;in\\;document\\;d}{Total\\;number\\;of\\;terms\\;in\\;the\\;document\\;d}$$\n",
    "\n",
    "\n",
    "`IDF` (inverse document frequency) đo đếm sự quan trọng của một từ, thuật ngữ trong kho văn bản. Trong việc tính toán TF, tất cả các terms được coi là quan trọng như nhau. Tuy nhiên có thể thấy rằng ví du trong tiếng anh thì các stop words như is, are, am, ... không hề quan trọng, mặc dù vậy chúng xuất hiện rất nhiều, và thường xuyên. Để giải quyết vấn đề này, IDF đánh trọng số thấp cho các terms mà rất phổ biến trong kho văn bản và đánh trọng số cao cho các terms hiếm gặp. Ta có công thức:\n",
    "\n",
    "$$IDF(t) = log_e(\\frac{Total\\;number\\;of\\;documents\\;in\\;the\\;corpus}{Number\\;of\\;documents\\;with\\;term\\;t\\;in\\;them})$$\n",
    "\n",
    "Cuối cùng TF-IDF là kết quả của tích giữa TF và IDF:\n",
    "\n",
    "$$TF-IDF = TF * IDF$$\n",
    "\n",
    "Ví dụ tính toán TF-IDF cho các từ trong kho văn bản của các ví dụ trước:\n",
    "\n",
    "![](../assets/images/19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Như vậy một documents sẽ được biểu diễn giống như các ví dụ trc là vector có số chiều bằng số từ trong vocabulary. Như với D1 là \"Dog bites man\" thì có biểu diễn như sau:\n",
    "\n",
    "![](../assets/images/20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ví dụ với sklearn. Trong ví dụ dưới ta thấy kết quả ra có vẻ khác với công thức ở trên lý do vì sklearn sử dụng cách tính khác một chút đó là để tránh trường hợp chia cho 0 (khi số lần xuất hiện của từ trong tất cả các docs = 0) đồng thời tránh idf = 0 nên công thức của họ là:\n",
    "\n",
    "$$idf(t) = \\log(\\frac{1 + n}{1 + df(t)}) + 1$$\n",
    "\n",
    "Trong đó:\n",
    "\n",
    "- n là số documents\n",
    "- df(t) là số lần xuất hiện của từ t trong tất cả các documents\n",
    "- log được hiểu là logarit cơ số tự nhiên hay $log_e$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"]\n",
    "processed_docs = [doc.lower().replace(\".\",\"\") for doc in documents]\n",
    "processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF for all words in the vocabulary [1.51082562 1.22314355 1.51082562 1.91629073 1.22314355 1.91629073]\n",
      "----------\n",
      "All words in the vocabulary ['bites' 'dog' 'eats' 'food' 'man' 'meat']\n",
      "----------\n",
      "TFIDF representation for all documents in our corpus\n",
      " [[0.65782931 0.53256952 0.         0.         0.53256952 0.        ]\n",
      " [0.65782931 0.53256952 0.         0.         0.53256952 0.        ]\n",
      " [0.         0.44809973 0.55349232 0.         0.         0.70203482]\n",
      " [0.         0.         0.55349232 0.70203482 0.44809973 0.        ]]\n",
      "----------\n",
      "Tfidf representation for 'dog and man are friends':\n",
      " [[0.         0.70710678 0.         0.         0.70710678 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "bow_rep_tfidf = tfidf.fit_transform(processed_docs)\n",
    "\n",
    "#IDF for all words in the vocabulary\n",
    "print(\"IDF for all words in the vocabulary\",tfidf.idf_)\n",
    "print(\"-\"*10)\n",
    "#All words in the vocabulary.\n",
    "print(\"All words in the vocabulary\",tfidf.get_feature_names_out())\n",
    "print(\"-\"*10)\n",
    "\n",
    "#TFIDF representation for all documents in our corpus \n",
    "print(\"TFIDF representation for all documents in our corpus\\n\",bow_rep_tfidf.toarray()) \n",
    "print(\"-\"*10)\n",
    "\n",
    "temp = tfidf.transform([\"dog and man are friends\"])\n",
    "print(\"Tfidf representation for 'dog and man are friends':\\n\", temp.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK tổng hợp lại tất cả các phương pháp represent text đã nêu trên ta thấy:\n",
    "\n",
    "- Chúng đều là biểu diễn rời rạc, coi (các từ, n-grams) là units. Việc biểu diễn rời rạc như vậy làm cản trở khả năng nắm bắt mối quan hệ tương quan giữa các từ.\n",
    "- Việc biểu diễn như thế này có thể gặp trường hợp bị nhiều giá trị 0 trong vector representations\n",
    "- Không handle được OOV words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Distributed Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phương pháp này sinh ra để khắc phục các hạn chế của những phương pháp represent bằng vector đã nêu. Dùng neural network để tạo ra dense, low-dimensional representations của các từ. Ta có một số key terms cần hiểu sau:\n",
    "\n",
    "`Distributional similarity`\n",
    "\n",
    "- ý tưởng này có thể được hiểu là bây giờ nghĩa của các từ không chỉ được hiểu một cách độc lập nữa mà sẽ được hiểu theo ngữ cảnh của nó. Ví dụ trong cụm \"NLP rocks\". Nghĩa hiểu độc lập của từ \"rocks\" là hòn đá tuy nhiên đặt trong ngữa cảnh của cụm từ trên thì nó được hiểu là một thứ gì đó tốt và xịn xò.\n",
    "\n",
    "`Distributional hypothesis`\n",
    "\n",
    "- Về mặt ngữ nghĩa, giả thuyết này nói rằng các từ được dùng trong ngữ cảnh giống nhau có ý nghĩa giống nhau. Ví dụ 2 từ \"dog\" và \"cat\" dùng trong ngữ cảnh giống nhau thì theo giả thuyết của ta thì chúng sẽ có sự tương đồng lớn về mặt ý nghĩa. Xem xét cách biểu diễn bằng vector (VSM) thì nghĩa của từ biểu diễn bằng vector vậy nếu 2 từ sử dụng trong ngữ cảnh giống nhau thì biểu diễn vector của chúng có khoảng cách gần nhau.\n",
    "\n",
    "`Distributional representation`\n",
    "\n",
    "- Dựa trên Distributional hypothesis, kiểu biểu diễn dựa trên số lượng từ đc dùng trong các ngữ cảnh mà nó được dùng. Nói ngắn gọn như ví dụ các cách biểu diễn vector space one-hot, BoW, BoN hay TF-IDF chính là cách biểu diễn này\n",
    "\n",
    "`Distributed representation`\n",
    "\n",
    "- Vẫn dựa trên Distributional hypothesis. Ta thấy rằng kiểu biểu diễn Distributional representation bị hạn chế về high dimensional và sparse. Để khắc phục vấn đề đó thì distributed representation nén chiều dữ liệu lại.\n",
    "\n",
    "\n",
    "Có thể có chút confused giữa `distributional representation và distributed representation` do cách viết của chúng, nhưng chungs là 2 cách biểu diễn khác nhau. Có thể hiểu đơn giản...\n",
    "\n",
    "`Embedding`\n",
    "\n",
    "- Với tập các từ trong kho văn bản. Có thể hiểu embedding là sự mapping từ vector space kiểu distributional representations thành vector space representations kiểu distributed representation\n",
    "\n",
    "`Vector semantics`\n",
    "\n",
    "- Liên quan đến các NLP methods dùng để học cách biểu diễn từ dựa trên distributional properties của từ trong kho văn bản lớn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Một số mô hình pretrained embeddings là:\n",
    "- `Word2vec` của Google\n",
    "- `GloVe` của Stanford\n",
    "- `fasttext embeddings` của Facebook\n",
    "\n",
    "Dimension của chúng dao động tầm 25, 50, 100, 200, 300, 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wget\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "gn_vec_path = \"GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "# # wget.download(\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\")\n",
    "# gn_vec_zip_path = \"GoogleNews-vectors-negative300.bin.gz\"\n",
    "\n",
    "# with gzip.open(gn_vec_zip_path, 'rb') as f_in:\n",
    "#     with open(gn_vec_path, 'wb') as f_out:\n",
    "#         shutil.copyfileobj(f_in, f_out)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import psutil\n",
    "process = psutil.Process(os.getpid())\n",
    "from psutil import virtual_memory\n",
    "mem = virtual_memory()\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used in GB before Loading the Model: 0.14\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.05 seconds taken to load\n",
      "----------\n",
      "Finished loading Word2Vec\n",
      "----------\n",
      "Memory used in GB after Loading the Model: 4.83\n",
      "----------\n",
      "Percentage increase in memory usage: 3529.79% \n",
      "----------\n",
      "Numver of words in vocablulary:  3000000\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "pretrainedpath = gn_vec_path\n",
    "\n",
    "#Load W2V model. This will take some time, but it is a one time effort! \n",
    "pre = process.memory_info().rss\n",
    "print(\"Memory used in GB before Loading the Model: %0.2f\"%float(pre/(10**9))) #Check memory usage before loading the model\n",
    "print('-'*10)\n",
    "\n",
    "start_time = time.time() #Start the timer\n",
    "ttl = mem.total #Toal memory available\n",
    "\n",
    "w2v_model = KeyedVectors.load_word2vec_format(pretrainedpath, binary=True) #load the model\n",
    "print(\"%0.2f seconds taken to load\"%float(time.time() - start_time)) #Calculate the total time elapsed since starting the timer\n",
    "print('-'*10)\n",
    "\n",
    "print('Finished loading Word2Vec')\n",
    "print('-'*10)\n",
    "\n",
    "post = process.memory_info().rss\n",
    "print(\"Memory used in GB after Loading the Model: {:.2f}\".format(float(post/(10**9)))) #Calculate the memory used after loading the model\n",
    "print('-'*10)\n",
    "\n",
    "print(\"Percentage increase in memory usage: {:.2f}% \".format(float((post/pre)*100))) #Percentage increase in memory after loading the model\n",
    "print('-'*10)\n",
    "\n",
    "print(\"Numver of words in vocablulary: \",len(w2v_model.vocab)) #Number of words in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gorgeous', 0.8353004455566406),\n",
       " ('lovely', 0.810693621635437),\n",
       " ('stunningly_beautiful', 0.7329413890838623),\n",
       " ('breathtakingly_beautiful', 0.7231341004371643),\n",
       " ('wonderful', 0.6854087114334106),\n",
       " ('fabulous', 0.6700063943862915),\n",
       " ('loveliest', 0.6612576246261597),\n",
       " ('prettiest', 0.6595001816749573),\n",
       " ('beatiful', 0.6593326330184937),\n",
       " ('magnificent', 0.6591402292251587)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let us examine the model by knowing what the most similar words are, for a given word!\n",
    "w2v_model.most_similar('beautiful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.07421875e-01, -2.01171875e-01,  1.23046875e-01,  2.11914062e-01,\n",
       "       -9.13085938e-02,  2.16796875e-01, -1.31835938e-01,  8.30078125e-02,\n",
       "        2.02148438e-01,  4.78515625e-02,  3.66210938e-02, -2.45361328e-02,\n",
       "        2.39257812e-02, -1.60156250e-01, -2.61230469e-02,  9.71679688e-02,\n",
       "       -6.34765625e-02,  1.84570312e-01,  1.70898438e-01, -1.63085938e-01,\n",
       "       -1.09375000e-01,  1.49414062e-01, -4.65393066e-04,  9.61914062e-02,\n",
       "        1.68945312e-01,  2.60925293e-03,  8.93554688e-02,  6.49414062e-02,\n",
       "        3.56445312e-02, -6.93359375e-02, -1.46484375e-01, -1.21093750e-01,\n",
       "       -2.27539062e-01,  2.45361328e-02, -1.24511719e-01, -3.18359375e-01,\n",
       "       -2.20703125e-01,  1.30859375e-01,  3.66210938e-02, -3.63769531e-02,\n",
       "       -1.13281250e-01,  1.95312500e-01,  9.76562500e-02,  1.26953125e-01,\n",
       "        6.59179688e-02,  6.93359375e-02,  1.02539062e-02,  1.75781250e-01,\n",
       "       -1.68945312e-01,  1.21307373e-03, -2.98828125e-01, -1.15234375e-01,\n",
       "        5.66406250e-02, -1.77734375e-01, -2.08984375e-01,  1.76757812e-01,\n",
       "        2.38037109e-02, -2.57812500e-01, -4.46777344e-02,  1.88476562e-01,\n",
       "        5.51757812e-02,  5.02929688e-02, -1.06933594e-01,  1.89453125e-01,\n",
       "       -1.16210938e-01,  8.49609375e-02, -1.71875000e-01,  2.45117188e-01,\n",
       "       -1.73828125e-01, -8.30078125e-03,  4.56542969e-02, -1.61132812e-02,\n",
       "        1.86523438e-01, -6.05468750e-02, -4.17480469e-02,  1.82617188e-01,\n",
       "        2.20703125e-01, -1.22558594e-01, -2.55126953e-02, -3.08593750e-01,\n",
       "        9.13085938e-02,  1.60156250e-01,  1.70898438e-01,  1.19628906e-01,\n",
       "        7.08007812e-02, -2.64892578e-02, -3.08837891e-02,  4.06250000e-01,\n",
       "       -1.01562500e-01,  5.71289062e-02, -7.26318359e-03, -9.17968750e-02,\n",
       "       -1.50390625e-01, -2.55859375e-01,  2.16796875e-01, -3.63769531e-02,\n",
       "        2.24609375e-01,  8.00781250e-02,  1.56250000e-01,  5.27343750e-02,\n",
       "        1.50390625e-01, -1.14746094e-01, -8.64257812e-02,  1.19140625e-01,\n",
       "       -7.17773438e-02,  2.73437500e-01, -1.64062500e-01,  7.29370117e-03,\n",
       "        4.21875000e-01, -1.12792969e-01, -1.35742188e-01, -1.31835938e-01,\n",
       "       -1.37695312e-01, -7.66601562e-02,  6.25000000e-02,  4.98046875e-02,\n",
       "       -1.91406250e-01, -6.03027344e-02,  2.27539062e-01,  5.88378906e-02,\n",
       "       -3.24218750e-01,  5.41992188e-02, -1.35742188e-01,  8.17871094e-03,\n",
       "       -5.24902344e-02, -1.74713135e-03, -9.81445312e-02, -2.86865234e-02,\n",
       "        3.61328125e-02,  2.15820312e-01,  5.98144531e-02, -3.08593750e-01,\n",
       "       -2.27539062e-01,  2.61718750e-01,  9.86328125e-02, -5.07812500e-02,\n",
       "        1.78222656e-02,  1.31835938e-01, -5.35156250e-01, -1.81640625e-01,\n",
       "        1.38671875e-01, -3.10546875e-01, -9.71679688e-02,  1.31835938e-01,\n",
       "       -1.16210938e-01,  7.03125000e-02,  2.85156250e-01,  3.51562500e-02,\n",
       "       -1.01562500e-01, -3.75976562e-02,  1.41601562e-01,  1.42578125e-01,\n",
       "       -5.68847656e-02,  2.65625000e-01, -2.09960938e-01,  9.64355469e-03,\n",
       "       -6.68945312e-02, -4.83398438e-02, -6.10351562e-02,  2.45117188e-01,\n",
       "       -9.66796875e-02,  1.78222656e-02, -1.27929688e-01, -4.78515625e-02,\n",
       "       -7.26318359e-03,  1.79687500e-01,  2.78320312e-02, -2.10937500e-01,\n",
       "       -1.43554688e-01, -1.27929688e-01,  1.73339844e-02, -3.60107422e-03,\n",
       "       -2.04101562e-01,  3.63159180e-03, -1.19628906e-01, -6.15234375e-02,\n",
       "        5.93261719e-02, -3.23486328e-03, -1.70898438e-01, -3.14941406e-02,\n",
       "       -8.88671875e-02, -2.89062500e-01,  3.44238281e-02, -1.87500000e-01,\n",
       "        2.94921875e-01,  1.58203125e-01, -1.19628906e-01,  7.61718750e-02,\n",
       "        6.39648438e-02, -4.68750000e-02, -6.83593750e-02,  1.21459961e-02,\n",
       "       -1.44531250e-01,  4.54101562e-02,  3.68652344e-02,  3.88671875e-01,\n",
       "        1.45507812e-01, -2.55859375e-01, -4.46777344e-02, -1.33789062e-01,\n",
       "       -1.38671875e-01,  6.59179688e-02,  1.37695312e-01,  1.14746094e-01,\n",
       "        2.03125000e-01, -4.78515625e-02,  1.80664062e-02, -8.54492188e-02,\n",
       "       -2.48046875e-01, -3.39843750e-01, -2.83203125e-02,  1.05468750e-01,\n",
       "       -2.14843750e-01, -8.74023438e-02,  7.12890625e-02,  1.87500000e-01,\n",
       "       -1.12304688e-01,  2.73437500e-01, -3.26171875e-01, -1.77734375e-01,\n",
       "       -4.24804688e-02, -2.69531250e-01,  6.64062500e-02, -6.88476562e-02,\n",
       "       -1.99218750e-01, -7.03125000e-02, -2.43164062e-01, -3.66210938e-02,\n",
       "       -7.37304688e-02, -1.77734375e-01,  9.17968750e-02, -1.25000000e-01,\n",
       "       -1.65039062e-01, -3.57421875e-01, -2.85156250e-01, -1.66992188e-01,\n",
       "        1.97265625e-01, -1.53320312e-01,  2.31933594e-02,  2.06054688e-01,\n",
       "        1.80664062e-01, -2.74658203e-02, -1.92382812e-01, -9.61914062e-02,\n",
       "       -1.06811523e-02, -4.73632812e-02,  6.54296875e-02, -1.25732422e-02,\n",
       "        1.78222656e-02, -8.00781250e-02, -2.59765625e-01,  9.37500000e-02,\n",
       "       -7.81250000e-02,  4.68750000e-02, -2.22167969e-02,  1.86767578e-02,\n",
       "        3.11279297e-02,  1.04980469e-02, -1.69921875e-01,  2.58789062e-02,\n",
       "       -3.41796875e-02, -1.44042969e-02, -5.46875000e-02, -8.78906250e-02,\n",
       "        1.96838379e-03,  2.23632812e-01, -1.36718750e-01,  1.75781250e-01,\n",
       "       -1.63085938e-01,  1.87500000e-01,  3.44238281e-02, -5.63964844e-02,\n",
       "       -2.27689743e-05,  4.27246094e-02,  5.81054688e-02, -1.07910156e-01,\n",
       "       -3.88183594e-02, -2.69531250e-01,  3.34472656e-02,  9.81445312e-02,\n",
       "        5.63964844e-02,  2.23632812e-01, -5.49316406e-02,  1.46484375e-01,\n",
       "        5.93261719e-02, -2.19726562e-01,  6.39648438e-02,  1.66015625e-02,\n",
       "        4.56542969e-02,  3.26171875e-01, -3.80859375e-01,  1.70898438e-01,\n",
       "        5.66406250e-02, -1.04492188e-01,  1.38671875e-01, -1.57226562e-01,\n",
       "        3.23486328e-03, -4.80957031e-02, -2.48046875e-01, -6.20117188e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#What is the vector representation for a word? \n",
    "w2v_model['computer']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model['compute'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"word 'practicalnlp' not in vocabulary\"\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    #What if I am looking for a word that is not in this vocabulary?\n",
    "    print(w2v_model['practicalnlp'])\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cần lưu ý khi sử dụng pre-trained models:\n",
    "\n",
    "- Token/Words phải được lowercased, nếu không cũng sẽ bị word not found in vocabulary.\n",
    "- Không phải tất cả các từ đều có trong pre-trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sử dụng spacy để xem ví dụ lấy embedding representations cho full text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.0/en_core_web_md-3.7.0-py3-none-any.whl (42.8 MB)\n",
      "     ---------------------------------------- 42.8/42.8 MB 7.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in c:\\users\\hoang\\anaconda3\\lib\\site-packages (from en-core-web-md==3.7.0) (3.7.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hoang\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (21.3)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\hoang\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (1.21.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\hoang\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (2.28.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\hoang\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (1.1.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\hoang\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\hoang\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (0.9.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\hoang\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\hoang\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (5.2.1)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\hoang\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (0.3.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\hoang\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\hoang\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\hoang\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (2.4.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\hoang\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hoang\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (63.4.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\hoang\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\hoang\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (8.2.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\hoang\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hoang\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (2.11.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\hoang\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\hoang\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (4.64.1)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\hoang\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (0.10.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\hoang\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\hoang\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in c:\\users\\hoang\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\hoang\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\hoang\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hoang\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hoang\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hoang\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (2022.9.14)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\hoang\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\hoang\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (0.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\hoang\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\hoang\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.16.0,>=0.7.0 in c:\\users\\hoang\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (0.15.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\hoang\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (2.0.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "%time \n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "# process a sentence using the model\n",
    "mydoc = nlp(\"sơn\")\n",
    "#Get a vector for individual words\n",
    "#print(doc[0].vector) #vector for 'Canada', the first word in the text \n",
    "print(mydoc[0].vector) #Averaged vector for the entire sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Training our own embeddings\n",
    "\n",
    "- Continuous bag of words (CBOW)\n",
    "- SkipGram\n",
    "\n",
    "Đây là hai phương pháp tìm embedding cho từ với cách thực hiện đối lập nhau. Nhưng đều chung ý tưởng là dùng mạng neural network cơ bản 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CBOW`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đây là phương pháp tìm từ trung tâm dựa vào ngữ cảnh. Với mỗi từ trong câu được chọn làm từ trung tâm, từ ngữ cảnh là các từ trong câu có vị trí cách từ đích một khoảng không quá C/2 với C là một số tự nhiên dương. Như vậy với mỗi từ trung tâm, ta sẽ có một bộ không quá C từ ngữ cảnh.\n",
    "\n",
    "Ta sẽ tạo bộ dữ liệu training tên toàn bộ các câu của documents thuộc corpus. Mỗi sentence sẽ được dùng lấy dữ liệu như sau\n",
    "\n",
    "Ví dụ với C = 2 \n",
    "\n",
    "![](../assets/images/Screenshot%20from%202023-10-05%2013-53-39.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Như vậy với các training samples thì context là input và target là output cho mạng neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mạng CBOW biểu diễn như sau:\n",
    "\n",
    "![](../assets/images/Screenshot%20from%202023-10-05%2013-59-20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Khi đó Input là vector các từ context biểu diễn dưới dạng one-hot với số chiều bằng với số chiều của vocabulary và output là từ center có biểu diễn tương tự. Kết quả cho embedding của từ center sẽ là output của hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SkipGram`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ngược lại với `CBOW`, SkipGram là model tìm các từ context từ center word. Cách lấy dữ liệu:\n",
    "\n",
    "![](../assets/images/Screenshot%20from%202023-10-05%2014-03-11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kiến trúc mô hình\n",
    "\n",
    "![](../assets/images/Screenshot%20from%202023-10-05%2014-04-41.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement Skip-gram from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Going beyond words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Với các cách biểu diễn từ đã biết đều dùng cho từng từ đơn giản, vậy để biểu diễn cả câu thì làm như thế nào?\n",
    "\n",
    "Phương pháp đơn giản nhất là biểu diễn từng từ, sau đó kết hợp chúng bằng cách tính tổng, hay tính trung bình cộng. Phương pháp này thì có vẻ như không bắt được hết các thuộc tính của câu như thứ tự từ tuy nhiên trong thực tế thì nó việc sử dụng như vậy rất ok.\n",
    "\n",
    "Một vấn đề gặp phải với các phương pháp biểu diễn kể cả khi dùng word2vec, cả pre-trained hay self-trained thì đều dựa vào vocabulary có được trong quá trình training dữ liệu. Tuy nhiên khi chạy ứng dụng thực tế thì sẽ gặp phải nhiều trường hợp từ không nằm trong vocabulary hay OOV. Một phương pháp đơn giản để giải quyết vấn đề này đó là kệ các từ không có trong từ điển trong quá trình feature extraction.\n",
    "\n",
    "Một cách khác để giải quyết vấn đề về OOV là tạo vector represent cho các từ không có trong vocab một cách ngẫu nhiên với các giá trị nằm trong đoạn từ -0.25 đến 0.25. Việc này có thể giúp cải thiện hiệu suất của model tầm 1-2%\n",
    "\n",
    "Một cách khác để khắc phục OOV là như fastText của Facebook. Models sẽ break các từ ra thành các n-grams và biểu diễn embedding cho các n-grams này. KHi đó một từ sẽ có embedding là kết hợp của embedding của các n-grams của nó. Ví dụ với từ \"gregarious\" không nằm trong vocab, tuy nhiên với cách làm của fastText thì nó được break ra thành 'gre', 'reg', 'ega, ...'ous', Nên cuối cùng vẫn có present của gregarious nhờ sự kết hợp các embeddign của n-gram của nó"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Distributed Representations Beyond Words and Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Với các phương pháp dùng Word2vect vấn gặp phải vấn đề ngữ cảnh của toàn bộ câu - trật tự từ trong câu. Ví dụ: \"dog bites man\" và \"man bites dog\". Hai câu này nếu như biểu diễn sẽ y hệt nhau nhưng lại khác nhau về mặt ngữ nghĩa.\n",
    "\n",
    "Để giải quyết vấn đề nêu trên ta có kiểu biểu diễn khác nữa đó là Doc2Vect, lúc này ta sẽ không học từ các từ nữa mà học từ các cụm từ, các câu, đoạn văn hay toàn bộ văn bản.\n",
    "\n",
    "Doc2Vec dựa trên paragraph vectors frame"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
